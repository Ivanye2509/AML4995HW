{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607",
   "metadata": {
    "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607"
   },
   "source": [
    "# Applied Machine Learning Homework 4\n",
    "Due 12/15/21 11:59PM EST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df26be-5638-4b0d-a252-4437eb76aa46",
   "metadata": {
    "id": "70df26be-5638-4b0d-a252-4437eb76aa46"
   },
   "source": [
    "### Q1: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da8daa7-f0cf-47f8-bc5f-6eb13573891c",
   "metadata": {
    "id": "8da8daa7-f0cf-47f8-bc5f-6eb13573891c"
   },
   "source": [
    "We will train a supervised training model to predict if a tweet has a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d",
   "metadata": {
    "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d"
   },
   "source": [
    "#### Dataset loading & dev/test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7",
   "metadata": {
    "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7"
   },
   "source": [
    "1.1) Load the twitter dataset from NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
   "metadata": {
    "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
    "outputId": "1ddd96c2-a8e1-43dd-f17e-f5457fbc3bd9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Ivan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\twitter_samples.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('twitter_samples')\n",
    "from nltk.corpus import twitter_samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d62ce-3c78-4b6c-9238-111d990d170f",
   "metadata": {
    "id": "c41d62ce-3c78-4b6c-9238-111d990d170f"
   },
   "source": [
    "1.2) Load the positive & negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3",
   "metadata": {
    "id": "b92fb408-f72a-4c23-acd8-7c944a52edd3"
   },
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eae071-fd8a-4a46-9958-0525c635fd88",
   "metadata": {
    "id": "12eae071-fd8a-4a46-9958-0525c635fd88"
   },
   "source": [
    "1.3) Create a development & test split (80/20 ratio):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e92d47cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combine Datasets\n",
    "import pandas as pd\n",
    "df_positive = pd.DataFrame(all_positive_tweets)\n",
    "df_negative = pd.DataFrame(all_negative_tweets)\n",
    "df_positive['sentiment'] = \"positive\"\n",
    "df_negative['sentiment'] = \"negative\"\n",
    "df = pd.concat([df_positive,df_negative], ignore_index=True)\n",
    "df.rename(columns={0: 'tweets'}, inplace=True)\n",
    "sentiment = df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8",
   "metadata": {
    "id": "0f3673db-d7a8-470b-a3d3-f4522cd359b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    4009\n",
      "positive    3991\n",
      "Name: sentiment, dtype: int64\n",
      "positive    1009\n",
      "negative     991\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Train test split\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "dev_text,test_text,dev_y,test_y = train_test_split(df[\"tweets\"],df[\"sentiment\"], test_size = 0.2,random_state = 2021)\n",
    "print(dev_y.value_counts())\n",
    "print(test_y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d",
   "metadata": {
    "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d"
   },
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe84444-3839-42f6-99e6-33126f10df9b",
   "metadata": {
    "id": "abe84444-3839-42f6-99e6-33126f10df9b"
   },
   "source": [
    "We will do some data preprocessing before we tokenize the data. We will remove `#` symbol, hyperlinks, stop words & punctuations from the data. You can use the `re` package in python to find and replace these strings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310",
   "metadata": {
    "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310"
   },
   "source": [
    "1.4) Replace the `#` symbol with '' in every tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5db4dd6d-e775-49d3-96e1-57620c042d46",
   "metadata": {
    "id": "5db4dd6d-e775-49d3-96e1-57620c042d46"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "dev_text = [text.replace('#','\\\"') for text in dev_text]\n",
    "test_text = [text.replace('#','\\\"') for text in test_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe",
   "metadata": {
    "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe"
   },
   "source": [
    "1.5) Replace hyperlinks with '' in every tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "ff5a7411-df49-427b-adef-5e8e63224db0",
   "metadata": {
    "id": "ff5a7411-df49-427b-adef-5e8e63224db0"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "import re\n",
    "dev_text = [re.sub(r'http\\S+', '\\\"', text) for text in dev_text]\n",
    "test_text = [re.sub(r'http\\S+', '\\\"', text) for text in test_text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ae463-b611-4292-9ad2-b778856bf8bc",
   "metadata": {
    "id": "492ae463-b611-4292-9ad2-b778856bf8bc"
   },
   "source": [
    "1.6) Remove all stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "d177d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def removestop(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    stop_sentence = [word for word in token_words if word not in ENGLISH_STOP_WORDS]\n",
    "    return \" \".join(stop_sentence)\n",
    "\n",
    "dev_text = [removestop(text) for text in dev_text]\n",
    "test_text = [removestop(text) for text in test_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9",
   "metadata": {
    "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9"
   },
   "source": [
    "1.7) Remove all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e",
   "metadata": {
    "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "dev_text = [ re.sub(r'[^\\w\\s]', '', text) for text in dev_text ]\n",
    "test_text = [ re.sub(r'[^\\w\\s]', '', text) for text in test_text ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1af18-0c07-4ffb-994e-daead4740a53",
   "metadata": {
    "id": "b2f1af18-0c07-4ffb-994e-daead4740a53"
   },
   "source": [
    "1.8) Apply stemming on the development & test datasets using Porter algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "c84a52f6-a62a-4033-8d1d-239ff6904248",
   "metadata": {
    "id": "c84a52f6-a62a-4033-8d1d-239ff6904248"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def stemSentence(sentence):\n",
    "    token_words = word_tokenize(sentence)\n",
    "    stem_sentence = [porter.stem(word) for word in token_words]\n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "dev_text = [stemSentence(text) for text in dev_text]\n",
    "test_text = [stemSentence(text) for text in test_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e23ef-dafd-4183-b2f1-86089e281dd8",
   "metadata": {
    "id": "687e23ef-dafd-4183-b2f1-86089e281dd8"
   },
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef",
   "metadata": {
    "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef"
   },
   "source": [
    "1.9) Create bag of words features for each tweet in the development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a",
   "metadata": {
    "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ivan\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vector = CountVectorizer()\n",
    "dev_text = vector.fit_transform(dev_text)\n",
    "dev_text\n",
    "feature_names = vector.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e",
   "metadata": {
    "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e"
   },
   "source": [
    "1.10) Train a supervised learning model of choice on the development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
   "metadata": {
    "id": "3433a6b0-408d-462e-9072-3495b21bc97b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.899375"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code here\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "lr = LogisticRegressionCV(max_iter = 1000).fit(dev_text, dev_y)\n",
    "lr.score(dev_text,dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340",
   "metadata": {
    "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340"
   },
   "source": [
    "1.11) Create TF-IDF features for each tweet in the development dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510",
   "metadata": {
    "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 14511)"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Preprocess again and train the dev data set\n",
    "dev_text,test_text2,dev_y,test_y = train_test_split(df[\"tweets\"],df[\"sentiment\"], test_size = 0.2,random_state = 2021)\n",
    "#Replace the # symbol with '' in every tweet\n",
    "dev_text = [text.replace('#','\\\"') for text in dev_text]\n",
    "test_text2 = [text.replace('#','\\\"') for text in test_text2]\n",
    "#Replace hyperlinks with '' in every tweet\n",
    "dev_text = [re.sub(r'http\\S+', '\\\"', text) for text in dev_text]\n",
    "test_text2 = [re.sub(r'http\\S+', '\\\"', text) for text in test_text2]\n",
    "#Remove all punctuations\n",
    "dev_text = [ re.sub(r'[^\\w\\s]', '', text) for text in dev_text ]\n",
    "test_text2 = [ re.sub(r'[^\\w\\s]', '', text) for text in test_text2 ]\n",
    "#Apply stemming on the development & test datasets using Porter algorithm\n",
    "dev_text = [stemSentence(text) for text in dev_text]\n",
    "test_text2 = [stemSentence(text) for text in test_text2]\n",
    "\n",
    "## Using Soft Stop words\n",
    "vector2 = TfidfVectorizer(stop_words = \"english\")\n",
    "dev_text = vector2.fit_transform(dev_text)\n",
    "dev_text.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427",
   "metadata": {
    "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427"
   },
   "source": [
    "1.12) Train the same supervised learning algorithm on the development dataset with TF-IDF features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
   "metadata": {
    "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94625"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#code here\n",
    "lr2 = LogisticRegressionCV(max_iter = 1000).fit(dev_text, dev_y)\n",
    "lr2.score(dev_text,dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92",
   "metadata": {
    "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92"
   },
   "source": [
    "1.13) Compare the performance of the two models on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "a64ca176-dab8-4965-a85d-dcf9dc013717",
   "metadata": {
    "id": "a64ca176-dab8-4965-a85d-dcf9dc013717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score for test dataset using BOW: 0.7455\n",
      "Score for test dataset using TF-IDF: 0.7535\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "test1 = vector.transform(test_text)\n",
    "test2 = vector2.transform(test_text2)\n",
    "print(\"Score for test dataset using BOW:\", lr.score(test1, test_y))\n",
    "print(\"Score for test dataset using TF-IDF:\", lr2.score(test2, test_y))\n",
    "## Thus, we can see both models are overfitting, but the TF-IDF models still perform slightly better then the BOW model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "HW4-NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
